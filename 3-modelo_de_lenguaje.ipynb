{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588ccc60",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural I\n",
    "\n",
    "**Autor:** Gonzalo G. Fernandez\n",
    "\n",
    "Clase 3: Modelos secuenciales - RNNs y LSTM\n",
    "\n",
    "## Consigna desafío 3\n",
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "**Sugerencias:**\n",
    "\n",
    "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
    "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
    "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e8f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 17:16:40.624193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-20 17:16:40.748547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750432600.806588    8596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750432600.821101    8596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750432600.923159    8596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750432600.923175    8596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750432600.923177    8596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750432600.923177    8596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-20 17:16:40.935134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils import parse_srt_to_dialogue\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    SimpleRNN,\n",
    "    Dense,\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc141ed5",
   "metadata": {},
   "source": [
    "## Resolución\n",
    "\n",
    "Se utilizarán los diálogos de la película argentina \"Nueve Reinas\" del año 2000 como corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd32346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dialogues extracted: 1371\n",
      "¿Qué estás leyendo?\n",
      "Nada... disculpáme. No, está todo bien.\n",
      "¿Nada más que esto? Sí.\n",
      "Esta máquina me vuelve loca. Después lo registro.\n",
      "1.25 más 3.75 son... 5\n"
     ]
    }
   ],
   "source": [
    "dialogues = parse_srt_to_dialogue(\"data/nueve_reinas-subtitles.srt\")\n",
    "print(f\"Total dialogues extracted: {len(dialogues)}\")\n",
    "for d in dialogues[:5]:\n",
    "    print(d)\n",
    "corpus = \" \".join(dialogues).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400605d",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "\n",
    "Tokenización de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f3dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(corpus)))\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "tokenized_corpus = [char2idx[c] for c in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775aabb",
   "metadata": {},
   "source": [
    "Creación de secuencias de entrada y labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03778405",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "step = 1\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(tokenized_corpus) - seq_length, step):\n",
    "    sequences.append(tokenized_corpus[i : i + seq_length])\n",
    "    next_chars.append(tokenized_corpus[i + seq_length])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = np.array(next_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309f302",
   "metadata": {},
   "source": [
    "- División del dataset en entrenamiento y validación.\n",
    "- One-Hot encoding de las secuencias de entrada y los labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709d7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_train_encoded = to_categorical(X_train, num_classes=vocab_size)\n",
    "y_train_encoded = to_categorical(y_train, num_classes=vocab_size)\n",
    "\n",
    "X_test_encoded = to_categorical(X_test, num_classes=vocab_size)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75220e",
   "metadata": {},
   "source": [
    "Se utiliza como métrica de desempeño la perplejidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea393ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    return K.exp(K.mean(cross_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2d338",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de modelo SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba19cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.2402 - loss: 2.8092 - perplexity: 17.6803 - val_accuracy: 0.3320 - val_loss: 2.2948 - val_perplexity: 10.0416\n",
      "Epoch 2/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.3394 - loss: 2.2232 - perplexity: 9.3766 - val_accuracy: 0.3637 - val_loss: 2.1440 - val_perplexity: 8.6480\n",
      "Epoch 3/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.3672 - loss: 2.0904 - perplexity: 8.1897 - val_accuracy: 0.3755 - val_loss: 2.0915 - val_perplexity: 8.2094\n",
      "Epoch 4/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.3819 - loss: 2.0320 - perplexity: 7.7285 - val_accuracy: 0.3747 - val_loss: 2.0655 - val_perplexity: 7.9849\n",
      "Epoch 5/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.3959 - loss: 1.9913 - perplexity: 7.4332 - val_accuracy: 0.3847 - val_loss: 2.0363 - val_perplexity: 7.7799\n",
      "Epoch 6/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4023 - loss: 1.9396 - perplexity: 7.0504 - val_accuracy: 0.3873 - val_loss: 2.0012 - val_perplexity: 7.5023\n",
      "Epoch 7/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4135 - loss: 1.9153 - perplexity: 6.8902 - val_accuracy: 0.3924 - val_loss: 1.9957 - val_perplexity: 7.4743\n",
      "Epoch 8/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4161 - loss: 1.8949 - perplexity: 6.7334 - val_accuracy: 0.4029 - val_loss: 1.9596 - val_perplexity: 7.2062\n",
      "Epoch 9/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4288 - loss: 1.8572 - perplexity: 6.4861 - val_accuracy: 0.4113 - val_loss: 1.9472 - val_perplexity: 7.1161\n",
      "Epoch 10/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.4377 - loss: 1.8309 - perplexity: 6.3179 - val_accuracy: 0.4191 - val_loss: 1.9226 - val_perplexity: 6.9351\n",
      "Epoch 11/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.4504 - loss: 1.7957 - perplexity: 6.1112 - val_accuracy: 0.4229 - val_loss: 1.9191 - val_perplexity: 6.9189\n",
      "Epoch 12/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.4570 - loss: 1.7607 - perplexity: 5.9036 - val_accuracy: 0.4262 - val_loss: 1.9011 - val_perplexity: 6.7940\n",
      "Epoch 13/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4646 - loss: 1.7357 - perplexity: 5.7481 - val_accuracy: 0.4245 - val_loss: 1.9062 - val_perplexity: 6.8452\n",
      "Epoch 14/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4686 - loss: 1.7246 - perplexity: 5.6960 - val_accuracy: 0.4312 - val_loss: 1.8833 - val_perplexity: 6.6766\n",
      "Epoch 15/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4773 - loss: 1.6929 - perplexity: 5.5061 - val_accuracy: 0.4357 - val_loss: 1.8736 - val_perplexity: 6.6141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b8ad92f5630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_simple_rnn = Sequential(\n",
    "    [\n",
    "        SimpleRNN(128, input_shape=(seq_length, vocab_size)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_simple_rnn.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\", perplexity],\n",
    ")\n",
    "\n",
    "model_simple_rnn.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c9a1f",
   "metadata": {},
   "source": [
    "- Dado que el vocabulario es de 54, una perplejidad de ~6 indica relativamente buen desempeño.\n",
    "- El entrenamiento del modelo es relativamente rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69162be5",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cb3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.2075 - loss: 2.9605 - perplexity: 20.4029 - val_accuracy: 0.2896 - val_loss: 2.4689 - val_perplexity: 11.9628\n",
      "Epoch 2/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 39ms/step - accuracy: 0.3198 - loss: 2.3277 - perplexity: 10.3874 - val_accuracy: 0.3313 - val_loss: 2.2385 - val_perplexity: 9.4906\n",
      "Epoch 3/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.3455 - loss: 2.1858 - perplexity: 9.0339 - val_accuracy: 0.3509 - val_loss: 2.1439 - val_perplexity: 8.6428\n",
      "Epoch 4/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.3710 - loss: 2.0884 - perplexity: 8.1645 - val_accuracy: 0.3758 - val_loss: 2.0705 - val_perplexity: 8.0342\n",
      "Epoch 5/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.3845 - loss: 2.0253 - perplexity: 7.6876 - val_accuracy: 0.3880 - val_loss: 2.0149 - val_perplexity: 7.6031\n",
      "Epoch 6/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.3997 - loss: 1.9735 - perplexity: 7.2882 - val_accuracy: 0.3979 - val_loss: 1.9789 - val_perplexity: 7.3352\n",
      "Epoch 7/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.4169 - loss: 1.9132 - perplexity: 6.8840 - val_accuracy: 0.4078 - val_loss: 1.9351 - val_perplexity: 7.0193\n",
      "Epoch 8/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.4192 - loss: 1.8854 - perplexity: 6.6686 - val_accuracy: 0.4157 - val_loss: 1.9046 - val_perplexity: 6.8078\n",
      "Epoch 9/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 42ms/step - accuracy: 0.4335 - loss: 1.8424 - perplexity: 6.3944 - val_accuracy: 0.4273 - val_loss: 1.8698 - val_perplexity: 6.5798\n",
      "Epoch 10/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.4459 - loss: 1.7962 - perplexity: 6.1229 - val_accuracy: 0.4348 - val_loss: 1.8467 - val_perplexity: 6.4309\n",
      "Epoch 11/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.4533 - loss: 1.7712 - perplexity: 5.9639 - val_accuracy: 0.4452 - val_loss: 1.8213 - val_perplexity: 6.2685\n",
      "Epoch 12/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.4644 - loss: 1.7269 - perplexity: 5.7073 - val_accuracy: 0.4484 - val_loss: 1.7968 - val_perplexity: 6.1240\n",
      "Epoch 13/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.4759 - loss: 1.7010 - perplexity: 5.5601 - val_accuracy: 0.4501 - val_loss: 1.7864 - val_perplexity: 6.0655\n",
      "Epoch 14/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.4894 - loss: 1.6467 - perplexity: 5.2576 - val_accuracy: 0.4592 - val_loss: 1.7678 - val_perplexity: 5.9594\n",
      "Epoch 15/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.5006 - loss: 1.6233 - perplexity: 5.1351 - val_accuracy: 0.4648 - val_loss: 1.7522 - val_perplexity: 5.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b8ad4539690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm = Sequential(\n",
    "    [\n",
    "        LSTM(128, input_shape=(seq_length, vocab_size)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_lstm.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\", perplexity],\n",
    ")\n",
    "\n",
    "model_lstm.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb39b5e",
   "metadata": {},
   "source": [
    "- Puede observarse como el tiempo de entrenamiento es mucho mayor que con SimpleRNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af243104",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de modelo GRU (Gated Recurrent Unit)\n",
    "En búsqueda de un mejor desempeño que con LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fafed540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.2348 - loss: 2.8658 - perplexity: 19.3814 - val_accuracy: 0.3297 - val_loss: 2.2547 - val_perplexity: 9.6526\n",
      "Epoch 2/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.3442 - loss: 2.1802 - perplexity: 8.9676 - val_accuracy: 0.3694 - val_loss: 2.1006 - val_perplexity: 8.2633\n",
      "Epoch 3/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.3752 - loss: 2.0611 - perplexity: 7.9687 - val_accuracy: 0.3900 - val_loss: 2.0094 - val_perplexity: 7.5660\n",
      "Epoch 4/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.3974 - loss: 1.9626 - perplexity: 7.2159 - val_accuracy: 0.4085 - val_loss: 1.9404 - val_perplexity: 7.0548\n",
      "Epoch 5/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.4194 - loss: 1.9016 - perplexity: 6.7905 - val_accuracy: 0.4262 - val_loss: 1.8978 - val_perplexity: 6.7648\n",
      "Epoch 6/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.4359 - loss: 1.8239 - perplexity: 6.2855 - val_accuracy: 0.4318 - val_loss: 1.8527 - val_perplexity: 6.4645\n",
      "Epoch 7/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.4516 - loss: 1.7789 - perplexity: 6.0065 - val_accuracy: 0.4420 - val_loss: 1.8188 - val_perplexity: 6.2655\n",
      "Epoch 8/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.4654 - loss: 1.7296 - perplexity: 5.7065 - val_accuracy: 0.4600 - val_loss: 1.7801 - val_perplexity: 6.0259\n",
      "Epoch 9/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.4840 - loss: 1.6801 - perplexity: 5.4463 - val_accuracy: 0.4681 - val_loss: 1.7498 - val_perplexity: 5.8438\n",
      "Epoch 10/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.4999 - loss: 1.6235 - perplexity: 5.1394 - val_accuracy: 0.4700 - val_loss: 1.7330 - val_perplexity: 5.7540\n",
      "Epoch 11/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.5086 - loss: 1.5946 - perplexity: 4.9929 - val_accuracy: 0.4770 - val_loss: 1.7112 - val_perplexity: 5.6385\n",
      "Epoch 12/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.5201 - loss: 1.5526 - perplexity: 4.7882 - val_accuracy: 0.4874 - val_loss: 1.6970 - val_perplexity: 5.5560\n",
      "Epoch 13/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.5378 - loss: 1.5040 - perplexity: 4.5617 - val_accuracy: 0.4905 - val_loss: 1.6903 - val_perplexity: 5.5271\n",
      "Epoch 14/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.5465 - loss: 1.4663 - perplexity: 4.3944 - val_accuracy: 0.4917 - val_loss: 1.6827 - val_perplexity: 5.4900\n",
      "Epoch 15/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.5585 - loss: 1.4374 - perplexity: 4.2670 - val_accuracy: 0.4973 - val_loss: 1.6815 - val_perplexity: 5.4795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b8ad4bdb580>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru = Sequential(\n",
    "    [\n",
    "        GRU(128, input_shape=(seq_length, vocab_size)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_gru.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\", perplexity],\n",
    ")\n",
    "\n",
    "model_gru.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbfd00",
   "metadata": {},
   "source": [
    "### Comparación de perplejidad de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd00d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN Test Perplexity: 6.7452 (vocab_size=54)\n",
      "LSTM Test Perplexity: 5.9821 (vocab_size=54)\n",
      "GRU Test Perplexity: 5.5973 (vocab_size=54)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_perplexity = model_simple_rnn.evaluate(X_test_encoded, y_test_encoded, verbose=0)\n",
    "print(f\"SimpleRNN Test Perplexity: {test_perplexity:.4f} (vocab_size={vocab_size})\")\n",
    "\n",
    "test_loss, test_acc, test_perplexity = model_lstm.evaluate(X_test_encoded, y_test_encoded, verbose=0)\n",
    "print(f\"LSTM Test Perplexity: {test_perplexity:.4f} (vocab_size={vocab_size})\")\n",
    "\n",
    "test_loss, test_acc, test_perplexity = model_gru.evaluate(X_test_encoded, y_test_encoded, verbose=0)\n",
    "print(f\"GRU Test Perplexity: {test_perplexity:.4f} (vocab_size={vocab_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcece53",
   "metadata": {},
   "source": [
    "Ordenando los modelos de forma decreciente utilizando como métrica la perplejidad en el subset de test se obtiene la siguiente lista:\n",
    "\n",
    "1. GRU\n",
    "2. LSTM\n",
    "3. SimpleRNN\n",
    "\n",
    "Es importante destacar que solo observando la perplejidad el desempeño es muy similar y por lo tanto resalta SimpleRNN como un modelo facil de entrenar que ofrece buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b8ce6",
   "metadata": {},
   "source": [
    "### Búsqueda de hiper parámetros para los modelos en estudio\n",
    "\n",
    "Implementación de función constructora del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462c1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    rnn_type = hp.Choice(\"rnn_type\", [\"SimpleRNN\", \"LSTM\", \"GRU\"])\n",
    "    units = hp.Int(\"units\", min_value=64, max_value=256, step=64)\n",
    "    dropout = hp.Float(\"dropout\", 0.0, 0.3, step=0.1)\n",
    "    learning_rate = hp.Choice(\"lr\", [1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    RNN = getattr(tf.keras.layers, rnn_type)\n",
    "    model.add(RNN(units, input_shape=(seq_length, vocab_size)))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[perplexity],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10f807",
   "metadata": {},
   "source": [
    "Definición del tuner para realizar búsquedas o para cargar parámetros ya exportados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4222057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from output/9reinas_rnn_tuning/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective=kt.Objective(\"val_perplexity\", direction=\"min\"),\n",
    "    max_epochs=10,\n",
    "    directory=\"output\",\n",
    "    project_name=\"9reinas_rnn_tuning\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df07d2d",
   "metadata": {},
   "source": [
    "Búsqueda de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62af5141",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m      2\u001b[0m     X_train_encoded,\n\u001b[1;32m      3\u001b[0m     y_train_encoded,\n\u001b[1;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test_encoded, y_test_encoded),\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80195c1e",
   "metadata": {},
   "source": [
    "Opcional: Carga de datos ya exportados (útil ante desconexión del notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d34af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1cc8e",
   "metadata": {},
   "source": [
    "Obtención de mejores hiperparámetros en la búsqueda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c416f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "rnn_type: LSTM\n",
      "units: 192\n",
      "dropout: 0.1\n",
      "lr: 0.01\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "print(\"Best hyperparameters:\")\n",
    "for hp_name in best_hps.values:\n",
    "    print(f\"{hp_name}: {best_hps.get(hp_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9667c92a",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e63c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell-1/workspace/ceia/ceia-nlp1/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750433030.945221   11633 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 2.4636 - perplexity: 13.5686 - val_loss: 1.9168 - val_perplexity: 7.0168\n",
      "Epoch 2/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.8361 - perplexity: 6.4843 - val_loss: 1.7499 - val_perplexity: 5.9827\n",
      "Epoch 3/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.6494 - perplexity: 5.3809 - val_loss: 1.6686 - val_perplexity: 5.5137\n",
      "Epoch 4/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.5465 - perplexity: 4.8519 - val_loss: 1.6254 - val_perplexity: 5.2860\n",
      "Epoch 5/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.4651 - perplexity: 4.4722 - val_loss: 1.6054 - val_perplexity: 5.1878\n",
      "Epoch 6/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.4013 - perplexity: 4.2120 - val_loss: 1.6024 - val_perplexity: 5.1761\n",
      "Epoch 7/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.3535 - perplexity: 4.0078 - val_loss: 1.6036 - val_perplexity: 5.1842\n",
      "Epoch 8/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.3263 - perplexity: 3.8975 - val_loss: 1.6253 - val_perplexity: 5.3571\n",
      "Epoch 9/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.2976 - perplexity: 3.7764 - val_loss: 1.6189 - val_perplexity: 5.3013\n",
      "Epoch 10/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.2577 - perplexity: 3.6405 - val_loss: 1.6368 - val_perplexity: 5.4014\n",
      "Epoch 11/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.2389 - perplexity: 3.5822 - val_loss: 1.6433 - val_perplexity: 5.4508\n",
      "Epoch 12/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.2230 - perplexity: 3.5102 - val_loss: 1.6335 - val_perplexity: 5.3904\n",
      "Epoch 13/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.1988 - perplexity: 3.4178 - val_loss: 1.7342 - val_perplexity: 6.0541\n",
      "Epoch 14/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.2034 - perplexity: 3.4557 - val_loss: 1.6945 - val_perplexity: 5.7969\n",
      "Epoch 15/15\n",
      "\u001b[1m1295/1295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 1.1712 - perplexity: 3.3415 - val_loss: 1.6872 - val_perplexity: 5.7578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x723d91e24bb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "model.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cbdee",
   "metadata": {},
   "source": [
    "### Generación de texto\n",
    "\n",
    "Implementación de greedy search para generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff08eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_greedy(model, seed, length=200):\n",
    "    seed = seed.lower()\n",
    "    generated = seed\n",
    "\n",
    "    for _ in range(length):\n",
    "        input_seq = [char2idx.get(c, 0) for c in seed[-seq_length:]]\n",
    "        input_seq = to_categorical([input_seq], num_classes=vocab_size)\n",
    "\n",
    "        preds = model.predict(input_seq, verbose=0)[0]\n",
    "\n",
    "        next_idx = np.argmax(preds) # greedy\n",
    "        next_char = idx2char[next_idx]\n",
    "\n",
    "        generated += next_char\n",
    "        seed += next_char\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5fa94a",
   "metadata": {},
   "source": [
    "Implementación de beam search para generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9341687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_beam_search(model, seed, length=200, beam_width=3):\n",
    "    seed = seed.lower()\n",
    "    sequences = [(seed, 0.0)]\n",
    "\n",
    "    for _ in range(length):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            input_seq = [char2idx.get(c, 0) for c in seq[-seq_length:]]\n",
    "            input_seq = to_categorical([input_seq], num_classes=vocab_size)\n",
    "            preds = model.predict(input_seq, verbose=0)[0]\n",
    "\n",
    "            top_indices = np.argsort(preds)[-beam_width:] # beam search\n",
    "\n",
    "            for idx in top_indices:\n",
    "                next_char = idx2char[idx]\n",
    "                prob = preds[idx]\n",
    "                candidate = (seq + next_char, score + np.log(prob + 1e-8))  # log-sum\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "    return sequences[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f56092",
   "metadata": {},
   "source": [
    "Comparación de greedy search para generación de texto entre modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "684f0e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN: la verdad es que las para vas a la mi rengunte de mi harmano. ¿no e\n",
      "LSTM: la verdad es que estás los estampillas. ¿qué querés? no, esto es un\n",
      "GRU: la verdad es que no tengo que tengo que lo que tengo que lo que te \n"
     ]
    }
   ],
   "source": [
    "print(\"SimpleRNN:\", generate_text_greedy(model_simple_rnn, \"la verdad es que \", length=50))\n",
    "print(\"LSTM:\", generate_text_beam_search(model_lstm, \"la verdad es que \", length=50))\n",
    "print(\"GRU:\", generate_text_beam_search(model_gru, \"la verdad es que \", length=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf77cf",
   "metadata": {},
   "source": [
    "Comparación de beam search para generación de texto entre modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43af12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN: la verdad es que tengo que tengo que tengo que tengo que tengo que \n",
      "LSTM: la verdad es que estás los estampillas. ¿qué querés? no, esto es un\n",
      "GRU: la verdad es que no tengo que tengo que lo que tengo que lo que te \n"
     ]
    }
   ],
   "source": [
    "print(\"SimpleRNN:\", generate_text_beam_search(model_simple_rnn, \"la verdad es que \", length=50))\n",
    "print(\"LSTM:\", generate_text_beam_search(model_lstm, \"la verdad es que \", length=50))\n",
    "print(\"GRU:\", generate_text_beam_search(model_gru, \"la verdad es que \", length=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65247d13",
   "metadata": {},
   "source": [
    "Puede observarse cómo las palabras se generan en su mayoría correctamente con los tres modelos pero en todos los casos la oración generada carece de contenido semántico. Además, hay una tendencia a generar palabras de gran frecuancia (comunes) en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeafdf7",
   "metadata": {},
   "source": [
    "## Ejecución del mejor modelo obtenido\n",
    "Generación de texto con el mejor modelo de los estudiados, resultado de la búsqueda de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dee1a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: mira que yo vende la plata. ¿quién es? ¿tía? escucheme, hablam\n",
      "LSTM: tomamos algo de mierda. ¿quién es? ¿tía? escucheme, hablamos es\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM:\", generate_text_beam_search(model, \"mira que yo \", length=50))\n",
    "print(\"LSTM:\", generate_text_beam_search(model, \"tomamos algo \", length=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
