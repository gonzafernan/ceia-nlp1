{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588ccc60",
   "metadata": {},
   "source": [
    "# Procesamiento de Lenguaje Natural I\n",
    "\n",
    "**Autor:** Gonzalo G. Fernandez\n",
    "\n",
    "Clase 3: Modelos secuenciales - RNNs y LSTM\n",
    "\n",
    "## Consigna desafío 3\n",
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "**Sugerencias:**\n",
    "\n",
    "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
    "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
    "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09e8f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_srt_to_dialogue\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    SimpleRNN,\n",
    "    Dense,\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc141ed5",
   "metadata": {},
   "source": [
    "## Resolución\n",
    "\n",
    "Se utilizarán los diálogos de la película argentina \"Nueve Reinas\" del año 2000 como corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd32346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dialogues extracted: 1371\n",
      "¿Qué estás leyendo?\n",
      "Nada... disculpáme. No, está todo bien.\n",
      "¿Nada más que esto? Sí.\n",
      "Esta máquina me vuelve loca. Después lo registro.\n",
      "1.25 más 3.75 son... 5\n"
     ]
    }
   ],
   "source": [
    "dialogues = parse_srt_to_dialogue(\"data/nueve_reinas-subtitles.srt\")\n",
    "print(f\"Total dialogues extracted: {len(dialogues)}\")\n",
    "for d in dialogues[:5]:\n",
    "    print(d)\n",
    "corpus = \" \".join(dialogues).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400605d",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "\n",
    "Tokenización de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f3dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(corpus)))\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "tokenized_corpus = [char2idx[c] for c in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775aabb",
   "metadata": {},
   "source": [
    "Creación de secuencias de entrada y labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03778405",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "step = 1\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(tokenized_corpus) - seq_length, step):\n",
    "    sequences.append(tokenized_corpus[i : i + seq_length])\n",
    "    next_chars.append(tokenized_corpus[i + seq_length])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = np.array(next_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309f302",
   "metadata": {},
   "source": [
    "- División del dataset en entrenamiento y validación.\n",
    "- One-Hot encoding de las secuencias de entrada y los labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709d7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_train_encoded = to_categorical(X_train, num_classes=vocab_size)\n",
    "y_train_encoded = to_categorical(y_train, num_classes=vocab_size)\n",
    "\n",
    "X_test_encoded = to_categorical(X_test, num_classes=vocab_size)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75220e",
   "metadata": {},
   "source": [
    "Se utiliza como métrica de desempeño la perplejidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea393ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    return K.exp(K.mean(cross_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2d338",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de modelo SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ba19cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggf/workspace/ceia/ceia-nlp1/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.2373 - loss: 2.8112 - perplexity: 17.7516 - val_accuracy: 0.3296 - val_loss: 2.2973 - val_perplexity: 10.0630\n",
      "Epoch 2/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.3468 - loss: 2.2141 - perplexity: 9.2785 - val_accuracy: 0.3625 - val_loss: 2.1467 - val_perplexity: 8.6727\n",
      "Epoch 3/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.3680 - loss: 2.0921 - perplexity: 8.2185 - val_accuracy: 0.3700 - val_loss: 2.0949 - val_perplexity: 8.2272\n",
      "Epoch 4/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.3855 - loss: 2.0192 - perplexity: 7.6324 - val_accuracy: 0.3831 - val_loss: 2.0424 - val_perplexity: 7.8207\n",
      "Epoch 5/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.3992 - loss: 1.9778 - perplexity: 7.3403 - val_accuracy: 0.3886 - val_loss: 2.0199 - val_perplexity: 7.6431\n",
      "Epoch 6/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.4053 - loss: 1.9440 - perplexity: 7.0860 - val_accuracy: 0.4008 - val_loss: 1.9831 - val_perplexity: 7.3732\n",
      "Epoch 7/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.4172 - loss: 1.8979 - perplexity: 6.7678 - val_accuracy: 0.4050 - val_loss: 1.9684 - val_perplexity: 7.2721\n",
      "Epoch 8/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.4233 - loss: 1.8677 - perplexity: 6.5618 - val_accuracy: 0.4055 - val_loss: 1.9580 - val_perplexity: 7.1981\n",
      "Epoch 9/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.4363 - loss: 1.8336 - perplexity: 6.3407 - val_accuracy: 0.4138 - val_loss: 1.9426 - val_perplexity: 7.0861\n",
      "Epoch 10/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.4532 - loss: 1.7994 - perplexity: 6.1412 - val_accuracy: 0.4133 - val_loss: 1.9225 - val_perplexity: 6.9491\n",
      "Epoch 11/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.4531 - loss: 1.7724 - perplexity: 5.9621 - val_accuracy: 0.4194 - val_loss: 1.9149 - val_perplexity: 6.9003\n",
      "Epoch 12/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.4617 - loss: 1.7531 - perplexity: 5.8499 - val_accuracy: 0.4203 - val_loss: 1.9133 - val_perplexity: 6.8921\n",
      "Epoch 13/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.4699 - loss: 1.7214 - perplexity: 5.6737 - val_accuracy: 0.4216 - val_loss: 1.9071 - val_perplexity: 6.8480\n",
      "Epoch 14/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.4746 - loss: 1.7010 - perplexity: 5.5529 - val_accuracy: 0.4222 - val_loss: 1.8951 - val_perplexity: 6.7721\n",
      "Epoch 15/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.4867 - loss: 1.6725 - perplexity: 5.4058 - val_accuracy: 0.4324 - val_loss: 1.8707 - val_perplexity: 6.6094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75703c2d9eb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_simple_rnn = Sequential(\n",
    "    [\n",
    "        SimpleRNN(128, input_shape=(seq_length, vocab_size)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_simple_rnn.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\", perplexity],\n",
    ")\n",
    "\n",
    "model_simple_rnn.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c9a1f",
   "metadata": {},
   "source": [
    "- Dado que el vocabulario es de 54, una perplejidad de ~6 indica relativamente buen desempeño.\n",
    "- El entrenamiento del modelo es relativamente rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69162be5",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cb3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggf/workspace/ceia/ceia-nlp1/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 91ms/step - accuracy: 0.2051 - loss: 2.9962 - perplexity: 21.4083 - val_accuracy: 0.3153 - val_loss: 2.3665 - val_perplexity: 10.7855\n",
      "Epoch 2/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 85ms/step - accuracy: 0.3158 - loss: 2.3149 - perplexity: 10.2629 - val_accuracy: 0.3452 - val_loss: 2.1764 - val_perplexity: 8.9133\n",
      "Epoch 3/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 110ms/step - accuracy: 0.3512 - loss: 2.1461 - perplexity: 8.6584 - val_accuracy: 0.3649 - val_loss: 2.1033 - val_perplexity: 8.2898\n",
      "Epoch 4/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 110ms/step - accuracy: 0.3715 - loss: 2.0685 - perplexity: 8.0215 - val_accuracy: 0.3752 - val_loss: 2.0417 - val_perplexity: 7.8011\n",
      "Epoch 5/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 104ms/step - accuracy: 0.3843 - loss: 2.0133 - perplexity: 7.5834 - val_accuracy: 0.3894 - val_loss: 1.9895 - val_perplexity: 7.4037\n",
      "Epoch 6/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 103ms/step - accuracy: 0.3990 - loss: 1.9580 - perplexity: 7.1937 - val_accuracy: 0.4037 - val_loss: 1.9471 - val_perplexity: 7.0951\n",
      "Epoch 7/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 77ms/step - accuracy: 0.4098 - loss: 1.9110 - perplexity: 6.8480 - val_accuracy: 0.4126 - val_loss: 1.9079 - val_perplexity: 6.8213\n",
      "Epoch 8/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 99ms/step - accuracy: 0.4206 - loss: 1.8756 - perplexity: 6.6044 - val_accuracy: 0.4228 - val_loss: 1.8826 - val_perplexity: 6.6539\n",
      "Epoch 9/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 107ms/step - accuracy: 0.4306 - loss: 1.8366 - perplexity: 6.3650 - val_accuracy: 0.4270 - val_loss: 1.8535 - val_perplexity: 6.4702\n",
      "Epoch 10/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 91ms/step - accuracy: 0.4454 - loss: 1.7863 - perplexity: 6.0517 - val_accuracy: 0.4344 - val_loss: 1.8239 - val_perplexity: 6.2798\n",
      "Epoch 11/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 105ms/step - accuracy: 0.4524 - loss: 1.7531 - perplexity: 5.8554 - val_accuracy: 0.4412 - val_loss: 1.8081 - val_perplexity: 6.1800\n",
      "Epoch 12/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 112ms/step - accuracy: 0.4598 - loss: 1.7276 - perplexity: 5.6970 - val_accuracy: 0.4548 - val_loss: 1.7795 - val_perplexity: 6.0111\n",
      "Epoch 13/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 116ms/step - accuracy: 0.4727 - loss: 1.6866 - perplexity: 5.4927 - val_accuracy: 0.4563 - val_loss: 1.7603 - val_perplexity: 5.8946\n",
      "Epoch 14/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 114ms/step - accuracy: 0.4852 - loss: 1.6436 - perplexity: 5.2400 - val_accuracy: 0.4589 - val_loss: 1.7519 - val_perplexity: 5.8441\n",
      "Epoch 15/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 114ms/step - accuracy: 0.4887 - loss: 1.6364 - perplexity: 5.1943 - val_accuracy: 0.4682 - val_loss: 1.7276 - val_perplexity: 5.7098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75707c3ec050>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm = Sequential(\n",
    "    [\n",
    "        LSTM(128, input_shape=(seq_length, vocab_size)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_lstm.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\", perplexity],\n",
    ")\n",
    "\n",
    "model_lstm.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb39b5e",
   "metadata": {},
   "source": [
    "- Puede observarse como el tiempo de entrenamiento es mucho mayor que con SimpleRNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af243104",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de modelo GRU (Gated Recurrent Unit)\n",
    "En búsqueda de un mejor desempeño que con LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fafed540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggf/workspace/ceia/ceia-nlp1/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 110ms/step - accuracy: 0.2329 - loss: 2.8720 - perplexity: 19.4547 - val_accuracy: 0.3395 - val_loss: 2.2389 - val_perplexity: 9.4813\n",
      "Epoch 2/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 106ms/step - accuracy: 0.3421 - loss: 2.1830 - perplexity: 9.0149 - val_accuracy: 0.3561 - val_loss: 2.1050 - val_perplexity: 8.3222\n",
      "Epoch 3/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 109ms/step - accuracy: 0.3705 - loss: 2.0717 - perplexity: 8.0656 - val_accuracy: 0.3858 - val_loss: 2.0120 - val_perplexity: 7.5826\n",
      "Epoch 4/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 106ms/step - accuracy: 0.3923 - loss: 1.9690 - perplexity: 7.2620 - val_accuracy: 0.4072 - val_loss: 1.9449 - val_perplexity: 7.0927\n",
      "Epoch 5/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 108ms/step - accuracy: 0.4183 - loss: 1.8931 - perplexity: 6.7330 - val_accuracy: 0.4216 - val_loss: 1.8960 - val_perplexity: 6.7504\n",
      "Epoch 6/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 113ms/step - accuracy: 0.4331 - loss: 1.8408 - perplexity: 6.3994 - val_accuracy: 0.4325 - val_loss: 1.8653 - val_perplexity: 6.5553\n",
      "Epoch 7/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 113ms/step - accuracy: 0.4465 - loss: 1.7939 - perplexity: 6.1007 - val_accuracy: 0.4452 - val_loss: 1.8172 - val_perplexity: 6.2428\n",
      "Epoch 8/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 96ms/step - accuracy: 0.4636 - loss: 1.7445 - perplexity: 5.8151 - val_accuracy: 0.4546 - val_loss: 1.7858 - val_perplexity: 6.0589\n",
      "Epoch 9/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 89ms/step - accuracy: 0.4806 - loss: 1.6773 - perplexity: 5.4307 - val_accuracy: 0.4693 - val_loss: 1.7609 - val_perplexity: 5.9056\n",
      "Epoch 10/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 88ms/step - accuracy: 0.4906 - loss: 1.6478 - perplexity: 5.2722 - val_accuracy: 0.4751 - val_loss: 1.7361 - val_perplexity: 5.7707\n",
      "Epoch 11/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 90ms/step - accuracy: 0.5038 - loss: 1.6001 - perplexity: 5.0261 - val_accuracy: 0.4803 - val_loss: 1.7148 - val_perplexity: 5.6530\n",
      "Epoch 12/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 88ms/step - accuracy: 0.5161 - loss: 1.5594 - perplexity: 4.8202 - val_accuracy: 0.4817 - val_loss: 1.7094 - val_perplexity: 5.6321\n",
      "Epoch 13/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 91ms/step - accuracy: 0.5291 - loss: 1.5213 - perplexity: 4.6379 - val_accuracy: 0.4899 - val_loss: 1.6906 - val_perplexity: 5.5274\n",
      "Epoch 14/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 89ms/step - accuracy: 0.5385 - loss: 1.4846 - perplexity: 4.4676 - val_accuracy: 0.4976 - val_loss: 1.6830 - val_perplexity: 5.4854\n",
      "Epoch 15/15\n",
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 89ms/step - accuracy: 0.5504 - loss: 1.4484 - perplexity: 4.3206 - val_accuracy: 0.5004 - val_loss: 1.6712 - val_perplexity: 5.4337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x757064717680>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru = Sequential(\n",
    "    [\n",
    "        GRU(128, input_shape=(seq_length, vocab_size)),\n",
    "        Dense(vocab_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_gru.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\", perplexity],\n",
    ")\n",
    "\n",
    "model_gru.fit(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbfd00",
   "metadata": {},
   "source": [
    "### Comparación de perplejidad de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd00d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN Test Perplexity: 6.5584 (vocab_size=54)\n",
      "LSTM Test Perplexity: 5.8016 (vocab_size=54)\n",
      "GRU Test Perplexity: 5.5559 (vocab_size=54)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_perplexity = model_simple_rnn.evaluate(X_test_encoded, y_test_encoded, verbose=0)\n",
    "print(f\"SimpleRNN Test Perplexity: {test_perplexity:.4f} (vocab_size={vocab_size})\")\n",
    "\n",
    "test_loss, test_acc, test_perplexity = model_lstm.evaluate(X_test_encoded, y_test_encoded, verbose=0)\n",
    "print(f\"LSTM Test Perplexity: {test_perplexity:.4f} (vocab_size={vocab_size})\")\n",
    "\n",
    "test_loss, test_acc, test_perplexity = model_gru.evaluate(X_test_encoded, y_test_encoded, verbose=0)\n",
    "print(f\"GRU Test Perplexity: {test_perplexity:.4f} (vocab_size={vocab_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcece53",
   "metadata": {},
   "source": [
    "Ordenando los modelos de forma decreciente utilizando como métrica la perplejidad en el subset de test se obtiene la siguiente lista:\n",
    "\n",
    "1. GRU\n",
    "2. LSTM\n",
    "3. SimpleRNN\n",
    "\n",
    "Es importante destacar que solo observando la perplejidad el desempeño es muy similar y por lo tanto resalta SimpleRNN como un modelo facil de entrenar que ofrece buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b8ce6",
   "metadata": {},
   "source": [
    "### Búsqueda de hiper parámetros para los modelos en estudio\n",
    "\n",
    "Implementación de función constructora del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462c1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    rnn_type = hp.Choice(\"rnn_type\", [\"SimpleRNN\", \"LSTM\", \"GRU\"])\n",
    "    units = hp.Int(\"units\", min_value=64, max_value=256, step=64)\n",
    "    dropout = hp.Float(\"dropout\", 0.0, 0.3, step=0.1)\n",
    "    learning_rate = hp.Choice(\"lr\", [1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    RNN = getattr(tf.keras.layers, rnn_type)\n",
    "    model.add(RNN(units, input_shape=(seq_length, vocab_size)))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[perplexity],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 01m 11s]\n",
      "val_perplexity: 19.270008087158203\n",
      "\n",
      "Best val_perplexity So Far: 18.206436157226562\n",
      "Total elapsed time: 00h 02m 28s\n",
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "GRU               |SimpleRNN         |rnn_type\n",
      "64                |64                |units\n",
      "0                 |0                 |dropout\n",
      "0.001             |0.0001            |lr\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "\u001b[1m353/648\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 53ms/step - loss: 3.1601 - perplexity: 25.6881"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective=kt.Objective(\"val_perplexity\", direction=\"min\"),\n",
    "    max_epochs=10,\n",
    "    directory=\"output\",\n",
    "    project_name=\"9reinas_rnn_tuning\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_encoded,\n",
    "    y_train_encoded,\n",
    "    validation_data=(X_test_encoded, y_test_encoded),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cbdee",
   "metadata": {},
   "source": [
    "### Generación de texto\n",
    "\n",
    "Implementación de greedy search para generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff08eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_greedy(model, seed, length=200):\n",
    "    seed = seed.lower()\n",
    "    generated = seed\n",
    "\n",
    "    for _ in range(length):\n",
    "        input_seq = [char2idx.get(c, 0) for c in seed[-seq_length:]]\n",
    "        input_seq = to_categorical([input_seq], num_classes=vocab_size)\n",
    "\n",
    "        preds = model.predict(input_seq, verbose=0)[0]\n",
    "\n",
    "        next_idx = np.argmax(preds) # greedy\n",
    "        next_char = idx2char[next_idx]\n",
    "\n",
    "        generated += next_char\n",
    "        seed += next_char\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5fa94a",
   "metadata": {},
   "source": [
    "Implementación de beam search para generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9341687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_beam_search(model, seed, length=200, beam_width=3):\n",
    "    seed = seed.lower()\n",
    "    sequences = [(seed, 0.0)]\n",
    "\n",
    "    for _ in range(length):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            input_seq = [char2idx.get(c, 0) for c in seq[-seq_length:]]\n",
    "            input_seq = to_categorical([input_seq], num_classes=vocab_size)\n",
    "            preds = model.predict(input_seq, verbose=0)[0]\n",
    "\n",
    "            top_indices = np.argsort(preds)[-beam_width:] # beam search\n",
    "\n",
    "            for idx in top_indices:\n",
    "                next_char = idx2char[idx]\n",
    "                prob = preds[idx]\n",
    "                candidate = (seq + next_char, score + np.log(prob + 1e-8))  # log-sum\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "    return sequences[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f56092",
   "metadata": {},
   "source": [
    "Comparación de greedy search para generación de texto entre modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "684f0e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN: la verdad es que te cama te para me raguna. ¿qué pasa? esta para qu\n",
      "LSTM: la verdad es que estás las pero no se lo que te vay a de puedo que \n",
      "GRU: la verdad es que no te lo que yo te lo que yo te lo que yo te lo qu\n"
     ]
    }
   ],
   "source": [
    "print(\"SimpleRNN:\", generate_text_greedy(model_simple_rnn, \"la verdad es que \", length=50))\n",
    "print(\"LSTM:\", generate_text_beam_search(model_lstm, \"la verdad es que \", length=50))\n",
    "print(\"GRU:\", generate_text_beam_search(model_gru, \"la verdad es que \", length=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf77cf",
   "metadata": {},
   "source": [
    "Comparación de beam search para generación de texto entre modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43af12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN: la verdad es que tengo que hacer una pertendo un para te hacer. ¿qu\n",
      "LSTM: la verdad es que estás las pero no se lo que te vay a de puedo que \n",
      "GRU: la verdad es que no te lo que yo te lo que yo te lo que yo te lo qu\n"
     ]
    }
   ],
   "source": [
    "print(\"SimpleRNN:\", generate_text_beam_search(model_simple_rnn, \"la verdad es que \", length=50))\n",
    "print(\"LSTM:\", generate_text_beam_search(model_lstm, \"la verdad es que \", length=50))\n",
    "print(\"GRU:\", generate_text_beam_search(model_gru, \"la verdad es que \", length=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65247d13",
   "metadata": {},
   "source": [
    "Puede observarse cómo las palabras se generan en su mayoría correctamente con los tres modelos pero en todos los casos la oración generada carece de contenido semántico. Además, hay una tendencia a generar palabras de gran frecuancia (comunes) en el corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
